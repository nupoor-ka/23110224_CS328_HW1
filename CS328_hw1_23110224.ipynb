{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVZj3NmrQ2T"
      },
      "source": [
        "# CS 328 Introduction to Data Science - Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3yq3Z3fra1Y"
      },
      "source": [
        "Author: Nupoor Assudani\n",
        "\n",
        "Roll number: 23110224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collaborators: Aryan Solanki 23110049"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_OPy27kTsViW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import struct\n",
        "from array import array\n",
        "%matplotlib inline\n",
        "import random\n",
        "from sklearn.metrics import rand_score\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import time\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gsL06sKrODf"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsBINNSRqDMt"
      },
      "source": [
        "Suppose you define a clustering objective in the following manner – give a partitioning $\\mathbb{C} = \\{C_1, . . . C_k\\}$,\n",
        "define $$\\text{cost}(\\mathbb{C}) = \\sum_{i} \\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2$$\n",
        "\n",
        "i.e. cost of a cluster is the sum of all pairwise squared distances. Give an algorithm for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT_ZmfnDwPU9"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CeWGG4gPsdxZ"
      },
      "outputs": [],
      "source": [
        "def calc_obj(X, centroids):  \n",
        "    x_reshaped = X[:, np.newaxis] # reshaping for the subtraction\n",
        "    distances = np.linalg.norm(x_reshaped - centroids, axis=2)  \n",
        "    labels = np.argmin(distances, axis=1)  # choosing the centroid each point is closest to as its cluster centre\n",
        "    obj = 0 \n",
        "    clusters = {i: [] for i in range(len(centroids))}  \n",
        "    for i, label in enumerate(labels):\n",
        "        clusters[label].append(X[i])\n",
        "    for cluster in clusters.values(): # iterating through each cluster\n",
        "        if len(cluster) > 1:\n",
        "            cl = np.array(cluster)\n",
        "            cl_reshaped = cl[:, np.newaxis] # reshaping for the subtraction\n",
        "            sum_sqrd_l2_norms = np.sum((cl_reshaped - cl) ** 2, axis=(1, 2)) # for each\n",
        "            obj += np.sum(sum_sqrd_l2_norms) / len(cluster)\n",
        "    return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.0\n"
          ]
        }
      ],
      "source": [
        "X_test = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "centroids_test = np.array([[2, 3], [4.5, 5.5]])\n",
        "print(calc_obj(X_test, centroids_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGt0T6eNrsyp"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd2VRairwb6"
      },
      "source": [
        "For the $k$-means problem, show that there is at most a factor of four ratio between the optimal value when\n",
        "we either require all cluster centers to be data points or allow arbitrary points to be centers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimal value when we allow cluster centres to be arbitrary points $$\\text{cost}(\\mathbb{C}) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2$$ where $\\mu_i$ is the centroid of $C_i$ or the mean of all points in $C_i$ and the clustering $\\mathbb{C} = \\{C_1, . . . C_k\\}$.\n",
        "\n",
        "The optimal value when we require cluster centres to be data points $$\\text{cost}(\\mathbb{C}_p) = \\sum_{i=1}^{k} \\sum_{x \\in C_{p_i}} \\|x - \\mu_{p_i}\\|_2^2$$ where $\\mu_{p_i}$ is the centroid of $C_{p_i}$ or the data point closest to the mean of all points in $C_{p_i}$ and the clustering $\\mathbb{C}_p = \\{C_{p_1}, . . . C_{p_k}\\}$.\n",
        "\n",
        "Here we assume that both the partitionings are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to prove that $\\text{cost}(\\mathbb{C}_p) \\leq 4 \\text{ cost}(\\mathbb{C})$.\n",
        "\n",
        "$\\text{Proof}$\n",
        "\n",
        "Using the triangle inequality, $$\\|x - \\mu_{p_i}\\|_2 \\leq \\|x - \\mu_i\\|_2 + \\|\\mu_i - \\mu_{p_i}\\|_2$$\n",
        "\n",
        "Squaring on both sides, $$\\|x - \\mu_{p_i}\\|_2^2 \\leq \\|x - \\mu_i\\|_2^2 + \\|\\mu_i - \\mu_{p_i}\\|_2^2 + 2 \\times \\|x - \\mu_i\\|_2 \\times \\|\\mu_i - \\mu_{p_i}\\|_2$$\n",
        "\n",
        "$\\mu_{p_i}$ is the point closest to $\\mu_i$ therefore for any value of $x$, $\\|\\mu_i - x\\|_2 \\geq \\|\\mu_{p_i} - \\mu_i\\|_2$.\n",
        "\n",
        "$$\\therefore \\|x - \\mu_{p_i}\\|_2^2 \\leq \\|x - \\mu_i\\|_2^2 + \\|\\mu_i - x\\|_2^2 + 2 \\times \\|x - \\mu_i\\|_2 \\times \\|\\mu_i - x\\|_2$$\n",
        "\n",
        "$$\\therefore \\|x - \\mu_{p_i}\\|_2^2 \\leq 4 \\|x - \\mu_i\\|_2^2$$\n",
        "\n",
        "$$\\therefore \\sum_{i=1}^{k} \\sum_{x \\in C_{p_i}}\\|x - \\mu_{p_i}\\|_2^2 \\leq \\sum_{i=1}^{k} \\sum_{x \\in C_i}4 \\|x - \\mu_i\\|_2^2$$\n",
        "\n",
        "$$\\text{cost}(\\mathbb{C}_p) \\leq 4\\text{ cost}(\\mathbb{C})$$\n",
        "\n",
        "This proves that for the $k$-means problem, there is at most a factor of four ratio between the optimal value when we either require all cluster centers to be data points or allow arbitrary points to be centers. $\\quad \\blacksquare$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsmbQrNiyE9E"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2cwT8ziyHN2"
      },
      "source": [
        "Create a random variable $X$ for which Markov’s inequality is tight. Give proof for your answer. If it is tight, then why are we using other inequalities e.g. Chebyshev and Chernoff?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Markov's inequality states that $ P(X \\geq t) \\leq \\frac{E[X]}{t}, \\quad \\text{for } X \\geq 0 \\text{ and } t > 0. $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is called tight when $ P(X \\geq t) = \\frac{E[X]}{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\n",
        "\n",
        "$$\n",
        "P(X = x) =\n",
        "\\begin{cases} \n",
        "\\frac{1}{2}, & \\text{if } x = 0 \\\\ \n",
        "\\frac{1}{2}, & \\text{if } x = 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This is a Bernoulli distribution with parameter \\( p = 0.5 \\):\n",
        "\n",
        "This can be written as $X \\sim \\text{Bernoulli}(p), \\quad \\text{where } p = 0.5$\n",
        "\n",
        "The expectation of X,\n",
        "$$\n",
        "\\mathbb{E}[X] = \\frac{1}{2} \\times 0 + \\frac{1}{2} \\times 1 = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "When $t=1$, Markov's inequality is tight in this case\n",
        "$$P(X=1) = \\frac{E[X]}{1} = \\frac{0.5}{1} = \\frac{1}{2}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even though Markov’s inequality can be tight, it is often loose for other distributions. This is because it only uses the expectation and ignores higher moments like variance.\n",
        "\n",
        "The advantage of Chebyshev's inequality when compared to Markov's are that it uses both the mean and variance giving a tighter bound for lower values of variance.\n",
        "\n",
        "The advantage of Chernoff's bound when compared to Markov's inequality is that it uses moment generating functions (i.e. function which gives all moments of the random variable), helping in measuring how \"spread out\" a random variable is. This allows us to get much sharper estimates on the probability of extreme events. It decreses exponentially, meaning the probability of a large deviation drops much faster.\n",
        "\n",
        "Thus we prefer bounds other than Markov for stronger results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlIvU4i7yJPZ"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHH4gis5ySxf"
      },
      "source": [
        "Download the MNIST dataset from https://www.kaggle.com/datasets/hojjatk/mnist-dataset/data. We will use the test dataset\n",
        "and test labels only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DO220Zd3SMXf"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# This is from a sample Notebook that demonstrates how to read \"MNIST Dataset\" from the site https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
        "# The author of the webpage is Hojjat Khodabakhsh\n",
        "#\n",
        "# MNIST Data Loader Class\n",
        "#\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, train_images_filepath, train_labels_filepath, test_images_filepath, test_labels_filepath):\n",
        "        self.train_images_filepath = train_images_filepath\n",
        "        self.train_labels_filepath = train_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())\n",
        "\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.train_images_filepath, self.train_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "TR5ePe7fSXFb",
        "outputId": "de60f59d-e5d7-4d13-f315-f77b3cc72817"
      },
      "outputs": [],
      "source": [
        "train_images_filepath = r\"C:\\Users\\nupoo\\Downloads\\archive\\train-images.idx3-ubyte\"\n",
        "train_labels_filepath = r\"C:\\Users\\nupoo\\Downloads\\archive\\train-labels.idx1-ubyte\"\n",
        "test_images_filepath = r\"C:\\Users\\nupoo\\Downloads\\archive\\t10k-images.idx3-ubyte\"\n",
        "test_labels_filepath = r\"C:\\Users\\nupoo\\Downloads\\archive\\t10k-labels.idx1-ubyte\"\n",
        "\n",
        "mnist_dataloader = MnistDataloader(train_images_filepath, train_labels_filepath, test_images_filepath, test_labels_filepath)\n",
        "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(a) Cluster them first using k-means clustering, $k$ = 10, with kmeans + + initialization (implement the\n",
        "complete Lloyd’s algorithm yourself). Check the Rand-index of the clustering against the true labels.\n",
        "Use the sklearn module for rand-index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kpp_init(X, k): # X (N, d), k int\n",
        "    c0 = X[np.random.choice(range(len(X)))]\n",
        "    centroids = [c0]\n",
        "    while len(centroids) < k:\n",
        "        x_reshaped = X[:, np.newaxis] # to (N, 1, D)\n",
        "        c_arr = np.array(centroids)\n",
        "        dist_mat = np.linalg.norm(x_reshaped - c_arr, axis = 2) # calculating euclidean distance, (N, k)\n",
        "        distances = np.min(dist_mat, axis=1) # array of distances from closest centre\n",
        "        dist_sqrd = distances**2 \n",
        "        dist_sum = np.sum(dist_sqrd)\n",
        "        probs = dist_sqrd/dist_sum # D(x_i)^2 / sum(D(x_i)^2)\n",
        "        new_c = X[np.random.choice(len(X), p=probs)] # chosen randomly with calculated probability\n",
        "        centroids.append(new_c) # new centroid added to list\n",
        "    return np.array(centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7tNAeMo7p5zz"
      },
      "outputs": [],
      "source": [
        "def kmeans(X, k, max_iters = 100, stop_change = 1e-4, random_state = 42):\n",
        "    np.random.seed(random_state)\n",
        "    centroids = kpp_init(X, k)\n",
        "    for i in range(max_iters):\n",
        "        x_reshaped = X[:, np.newaxis] # to (N, 1, D)\n",
        "        distances = np.linalg.norm(x_reshaped - centroids, axis = 2) # calculating euclidean distance, (N, k)\n",
        "        labels = np.argmin(distances, axis = 1) # assigned centroid that it is closest to (n,)\n",
        "        new_centroids = np.array([X[np.where(labels == i)].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)]) # current cluster means\n",
        "        if np.linalg.norm(new_centroids - centroids) < stop_change:\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "    return labels, centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = np.array(x_test)\n",
        "mnist_test = x_test.reshape(10000, 28 * 28)\n",
        "k = 10  # number of clusters 0-9\n",
        "labels, centroids = kmeans(mnist_test, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand index on performing k-means clustering with k-means ++ initialization: 0.8922220022002201\n"
          ]
        }
      ],
      "source": [
        "ri = rand_score(y_test, labels)\n",
        "print(f\"Rand index on performing k-means clustering with k-means ++ initialization: {ri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(b) Do the same for k-center clustering, $k$ = 10. Implement the greedy algorithm discussed in class.\n",
        "Report the Rand-index here too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def k_centre_greedy(X, k):\n",
        "    c0 = X[np.random.choice(range(len(X)))]\n",
        "    centroids = [c0]\n",
        "    for i in range(k-1):\n",
        "        x_reshaped = X[:, np.newaxis] # to (N, 1, D)\n",
        "        c_arr = np.array(centroids)\n",
        "        dist_mat = np.linalg.norm(x_reshaped - c_arr, axis = 2) # calculating euclidean distance, (N, k)\n",
        "        dist_from_c = np.min(dist_mat, axis = 1) # distances to closest centroid (n,)\n",
        "        new_c = X[np.argmax(dist_from_c)] # farthest point from the closest existing center\n",
        "        centroids.append(new_c)\n",
        "    x_reshaped = X[:, np.newaxis] # to (N, 1, D)\n",
        "    distances = np.linalg.norm(x_reshaped - np.array(centroids), axis = 2) # calculating euclidean distance, (N, k)\n",
        "    labels = np.argmin(distances, axis = 1) # assigned centroid that it is closest to (n,)\n",
        "    return labels, np.array(centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = np.array(x_test)\n",
        "mnist_test = x_test.reshape(10000, 28 * 28)\n",
        "k = 10  # number of clusters 0-9\n",
        "labels, centroids = k_centre_greedy(mnist_test, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand index on performing k-centres clustering with greedy approach: 0.5249141314131414\n"
          ]
        }
      ],
      "source": [
        "ri = rand_score(y_test, labels)\n",
        "print(f\"Rand index on performing k-centres clustering with greedy approach: {ri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(c) Run the single linkage agglomeration till there are $k$ = 10 clusters. Report Rand-index here too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def single_linkage_agglomeration(X, k):\n",
        "    clusters = {i:[i] for i in range(len(X))} # initially each point is in its own cluster\n",
        "    which_cluster = {i:i for i in range(len(X))}\n",
        "    n = len(X)\n",
        "    time1 = time.time()\n",
        "    pairwise_distances = squareform(pdist(X, metric='euclidean')) # precomputing distances for efficiency\n",
        "    heap = [(pairwise_distances[i, j], i, j) for i in range(n) for j in range(i+1, n)] # making a list of pairwise distances\n",
        "    heapq.heapify(heap) # min-heap created in place\n",
        "    # time2 = time.time()\n",
        "    # timer = time2 - time1\n",
        "    # print(f\"Precomputed pairwise distances for efficiency. Time taken = {timer}\")\n",
        "    # t = 0\n",
        "    while len(clusters)>k:\n",
        "        t1 = time.time()\n",
        "        while heap:\n",
        "            dist, i, j = heapq.heappop(heap) # the closest pair\n",
        "            c1, c2 = which_cluster[i], which_cluster[j]\n",
        "            if which_cluster[i] != which_cluster[j]: # if they don't belong to the same cluster, merge their clusters\n",
        "                break\n",
        "        clusters[c1].extend(clusters[c2]) # merging two clusters\n",
        "        for i in clusters[c2]:\n",
        "            which_cluster[i] = c1\n",
        "        del clusters[c2]\n",
        "        # t2 = time.time()\n",
        "        # t = t2 - t1\n",
        "        # print(f\"Current number of clusters is {len(clusters)}. Time taken for this iteration is {t}.\")\n",
        "    labels = np.zeros(len(X), dtype=int)\n",
        "    for cluster in list(clusters.keys()): # final labelling\n",
        "        for i in clusters[cluster]:\n",
        "            labels[i] = cluster    \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = np.array(x_test)\n",
        "mnist_test = x_test.reshape(10000, 28 * 28)\n",
        "k = 10  # number of clusters 0-9\n",
        "labels = single_linkage_agglomeration(mnist_test, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand index on performing single-linkage agglomeration: 0.1017039703970397\n"
          ]
        }
      ],
      "source": [
        "ri = rand_score(y_test, labels)\n",
        "print(f\"Rand index on performing single-linkage agglomeration: {ri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(d) Run the same algorithms (k-means and k-center) but on a rank-k approximation of the training data\n",
        "matrix. Note that if $A$ is the training data matrix (images × pixels), then you can just use $U_k Σ_k$ for\n",
        "the clustering, no need to use $V_k$. Evaluate for $k$ = 2, 5, 10 and report the rand-index values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand index on performing k-means clustering with k-means ++ initialization after svd with rank [2, 5, 10]: 0.5133793368778369\n",
            "Rand index on performing k-centres clustering with greedy approach after svd with rank [2, 5, 10]: 0.5119447063006606\n",
            "Rand index on performing k-means clustering with k-means ++ initialization after svd with rank [2, 5, 10]: 0.7538383450835292\n",
            "Rand index on performing k-centres clustering with greedy approach after svd with rank [2, 5, 10]: 0.7375379817441402\n",
            "Rand index on performing k-means clustering with k-means ++ initialization after svd with rank [2, 5, 10]: 0.8293026400440008\n",
            "Rand index on performing k-centres clustering with greedy approach after svd with rank [2, 5, 10]: 0.7827990055389812\n"
          ]
        }
      ],
      "source": [
        "k = [2, 5, 10]\n",
        "for k_val in k:\n",
        "    U_k, S_k, V_k = np.linalg.svd(x_train)\n",
        "    A_k = U_k @ np.diag(S_k)\n",
        "    labels1, centroids = kmeans(A_k, k_val)\n",
        "    ri1 = rand_score(y_train, labels1)\n",
        "    print(f\"Rand index on performing k-means clustering with k-means ++ initialization after svd with rank {k}: {ri1}\")\n",
        "    labels2, centroids = k_centre_greedy(A_k, k_val)\n",
        "    ri2 = rand_score(y_train, labels2)\n",
        "    print(f\"Rand index on performing k-centres clustering with greedy approach after svd with rank {k}: {ri2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_DNx-mIyL4S"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XpEFRLzyP_7"
      },
      "source": [
        "Suppose you have a population of 1 million people, out of which at least 1% are coffee drinkers. You want\n",
        "to get the estimate of this fraction by using sampling. Give the algorithm and the estimate. What kind\n",
        "of error bounds can you give with probability 99%?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The way to get the estimate for this is randomly sampling n people from the population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fraction of people who drink coffee can be defined as the sum of a number of Bernoulli random variables. Each random variable is a Bernoulli random variable, having the value 1 if the person drinks coffee and the value 0 if they don't.\n",
        "\n",
        "$S_n = X_1 + X_2 + ... + X_n$, where $X_i$ is the probability that the $i^{th}$ person we sample drinks coffee and we sample a total of n people.\n",
        "\n",
        "We know that at least 1% of the population drinks coffee, therefore $E[X_i] \\geq 0.01$ and $E[S_n] \\geq n\\times 0.01$.\n",
        "\n",
        "From the Chernoff bound, we know that\n",
        "$$P(S_n - E[S_n]\\geq t)\\leq e^{\\frac {-2t^2} {n}}$$\n",
        "$$P(|S_n - E[S_n]|\\geq t)\\leq 2e^{\\frac {-2t^2} {n}}$$\n",
        "\n",
        "We know that $E[X_i] = \\frac {1} {n} E[S_n]$.\n",
        "\n",
        "We want the probability of the error being within t to be 99%, therefore $$P(|S_n - E[S_n]|\\leq t) \\geq 0.99$$\n",
        "$$1 - P(|S_n - E[S_n]|\\leq t) \\leq 1 - 0.99$$\n",
        "$$P(|S_n - E[S_n]|\\geq t) \\leq 0.01$$\n",
        "\n",
        "Therefore, $$0.01 \\geq 2e^{\\frac {-2t^2} {n}}$$\n",
        "\n",
        "If t is the error in the calculation of $S_n$, and $\\epsilon$ is the error in the calculation of $X_i$, then we can also write $$0.01 \\geq 2e^{-2n\\epsilon^2}$$\n",
        "$$\\ln(0.005) \\geq -2n\\epsilon^2$$\n",
        "$$- \\frac{\\ln(0.005)}{2} \\leq n\\epsilon^2$$\n",
        "$$2.649 \\leq n\\epsilon^2$$\n",
        "$$n \\geq \\frac{2.649}{\\epsilon^2}$$\n",
        "\n",
        "Say the error bound is $\\pm 1%$, i.e., $\\epsilon = 0.01$,\n",
        "$$n \\geq \\frac{2.649}{0.01^2}$$\n",
        "$$n \\geq 26,491.5$$\n",
        "\n",
        "Therefore, we can say that for an error bound of 1%, n should be at least 26,492, i.e., at least 26,492 people should be sampled from a million."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
